* The `AWS_ACCESS_KEY` and `AWS_SECRET_KEY`:
  * These are tied to an [IAM User](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) (ie a person). In production, this chatbot code should use an IAM Role that has the appropriate permissions to use the Bedrock service.
  * If these keys are going to be used, they shouldn't be written in plaintext. They should be stored in a secrets manager such as [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html) or a Dockerfile/.env file or some other way that doesn't commit them to source control.
* The `while` loop is funny but maybe it's written this way for the sake of the exercise. In production, I wouldn't expect this chatbot code to directly except user input, run in a `while True` loop, or have the program quit when the user types "quit". There a lots of ways to implement this better but since this is code is running in AWS, a [Lambda function](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html) would be appropriate:
  * The chatbot code would only run when a request is sent to the Lambda function containing the user input.
  * Because a Lambda function is serverless, this means it handles [concurrency by creating new instances as needed](https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html). So you can expect it to handle ~100 request per minute during peak traffic.
* Sending raw user input to the LLM runs the risk of [prompt injection](https://www.datacamp.com/blog/prompt-injection-attack). There are ways to mitigate this such as using a prompt template and structured outputs to have more control on what gets sent to the LLM and received by it. There are also more [sophisticated guardrails](https://www.datacamp.com/blog/llm-guardrails) such as filtering for inappropriate language.
* The function `process_user_input` could be improved by stating the return type, such as `List[str]` The docstring could include that, too, as well as the `user_message` argument.
* The `BedrockChatbot` object is initialized each time a user sends a message. If the code is going to be continuously running, I would suggest making this a [singleton](https://www.geeksforgeeks.org/singleton-pattern-in-python-a-complete-guide/) that way it is a shared resouce. This should help with the < 2 second requirement.
* Having the LLM generate 10 responses to the same user input is a little odd. But what's even odder is displaying all 10 responses to the user. I suspect this wouldn't be a good user experience. To improve the user experience, I would suggest return one response (ideally the best response about the content they're asking about).
* Another way to improve the user experience is to stream the response from the LLM. Streaming each token as it's generated is the most common but you could also stream each message generated by the LLM.
* Another UX issue is the lack of chat history. This is ok if you don't want the user to have a back-and-forth conversation with the LLM. But if you do, then you'll want to maintain the state of the conversation to keep the history of messages. You'll also want to identify each user by an ID so that each request sent to the LLM contains only that user's message history.
* The prompt sent to the LLM could use improvement:
  * Right now there's no system message with instructions for the LLM (ie "you are to answer questions about the content of this page...")
  * The RAG piece is missing- you also need to send the page content this LLM is supposed to answer questions about.
  * The model `anthropic.claude-v2` is outdated (latest version is 4). This should be configurable such as making it an [environment variable in the Lambda function](https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html).
