* The `AWS_ACCESS_KEY` and `AWS_SECRET_KEY`:
  * These are tied to an [IAM User](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) (ie a person). In production, this chatbot code should use an IAM Role that has the appropriate permissions to use the Bedrock service.
  * If these keys are going to be used, they shouldn't be written in plaintext. They should be stored in a secrets manager such as [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html) or a Dockerfile/.env file or some other way that doesn't commit them to source control.
* The `while` loop is funny but maybe it's written this way for the sake of the exercise. In production, I wouldn't expect this chatbot code to directly except user input, run in a `while True` loop, or have the program quit when the user types "quit". There a lots of ways to implement this better but since this is code is running in AWS, a [Lambda function](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html) would be appropriate:
  * The chatbot code would only run when a request is sent to the Lambda function containing the user input.
  * Because a Lambda function is serverless, this means it handles [concurrency by creating new instances as needed](https://docs.aws.amazon.com/lambda/latest/dg/lambda-concurrency.html). So you can expect it to handle ~100 request per minute during peak traffic.
* Sending raw user input to the LLM runs the risk of [prompt injection](https://www.datacamp.com/blog/prompt-injection-attack). There are ways to mitigate this such as using a prompt template and structured outputs to have more control on what gets sent to the LLM and received by it. There are also more [sophisticated guardrails](https://www.datacamp.com/blog/llm-guardrails) such as filtering for inappropriate language.
* The function `process_user_input` could be improved by stating the return type, such as `List[str]` The docstring could include that, too, as well as the `user_message` argument.
* The `BedrockChatbot` object is initialized each time a user sends a message. If the code is going to be continuously running, I would suggest making this a [singleton](https://www.geeksforgeeks.org/singleton-pattern-in-python-a-complete-guide/) that way it is a shared resouce. This should help with the < 2 second requirement.
* Having the LLM generate 10 responses to the same user input is a little odd. But what's even odder is displaying all 10 responses to the user. I suspect this wouldn't be a good user experience. To improve the user experience, I would suggest return one response- ideally the best response about the content they're asking about or have an LLM summarize all 10 responses. If you do want to return all 10 messages, you could make this program asynchronous so that each message gets printed one-at-a-time instead of waiting for all 10 to finish.
* Another way to improve the user experience is to stream the response from the LLM. Streaming each token as it's generated is the most common but you could also stream each message generated by the LLM.
* Another UX issue is the lack of chat history. This is ok if you don't want the user to have a back-and-forth conversation with the LLM. But if you do, then you'll want to maintain the state of the conversation to keep the history of messages. You'll also want to identify each user by an ID so that each request sent to the LLM contains only that user's message history.
* The prompt sent to the LLM could use improvement:
  * Right now there's no system message with instructions for the LLM (ie "you are to answer questions about the content of this page...")
  * The RAG piece is missing- you also need to send the page content this LLM is supposed to answer questions about.
  * The model `anthropic.claude-v2` is outdated (latest version is 4). This should be configurable such as making it an [environment variable in the Lambda function](https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html).
  * The LLM temperature might be too high for answering factual questions about specific content. Setting it closer to zero would be my suggestion.
  * `max_tokens` to generate an answer might be too limiting. A short paragraph is about 500 tokens. My suggestion is to increase this but more importantly, give the LLM instructions and examples of what you want the output to be instead of relying on the maximum number of tokens it can generate. By giving the LLM examples you improve it's accuracy and by stating the output format (ie "generate 3 sentences" or use a [structured output](https://python.langchain.com/docs/concepts/structured_outputs/) you have more control over the answer it generates.
* The automatic retry needs improvement. Right now it's sending 5 requests even if the first one is successful. I would suggest removing this `for loop` and instead have a function that sends the request to the LLM. You can recursively call this function if the response has an error code. You can also implement an [exponential backoff](https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/retry-backoff.html) which might increase the chance for a successful response.
* The `try/catch` block is good to have but in a production system the error should be logged to a monitoring system (such as [AWS Cloud Watch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html)) so that it can be searched for by a developer. You may also want to setup alerts for too many errors (ie 10 errors in 10 minutes).
